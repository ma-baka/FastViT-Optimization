## 2025-11-19 (Day 1): 环境搭建与模型初步探索

### 1. 环境与数据排坑 (Setup & Debugging)
- **平台**: AutoDL (RTX 4090 D / 24GB VRAM)
- **环境修复**:
  - 解决 NumPy 2.x 与 PyTorch 1.11 不兼容报错，降级 NumPy。
  - 解决 DataLoader 报错：发现并删除了数据集中隐藏的 .ipynb_checkpoints 文件夹。
- **数据**: 成功配置 Imagenette (10类) 数据集。
- **工具**: 配置 GitHub SSH Key 实现代码同步；配置 .gitignore 过滤大文件。

### 2. 实验 A: 快速试错 (Baseline)
- **模型**: fastvit_t8 (Tiny版)
- **参数**: Batch Size 128, 从零训练 (Scratch)。
- **现象**:
  - 训练初期准确率极低 (~10%)。
  - **WandB 可视化**: 成功接入 WandB，观察到 train_loss 呈下降趋势，验证代码无 Bug，但模型收敛缓慢。

## 3. 实验 B: 性能压榨与模型升级 (Optimization)
- **模型**: `fastvit_ma36`
- **配置**: 
  - Batch Size: 64
  - Epochs: 100 + 10 (Cooldown)
  - 策略: 从零训练 (Training from Scratch)
- **最终结果 (Epoch 109)**:
  - **Top-1 准确率**: **87.18%**
  - **Top-5 准确率**: 98.73%
  - **Eval Loss**: 0.594
- **结果分析**:
  - 相比 T8 模型 (~10%) 提升巨大，证明了 MA36 架构在特征提取上的强悍能力。
  - 曲线显示在最后 10 个 Cooldown Epochs 阶段，准确率有显著拉升，证明移除数据增强后的微调策略非常有效。
  - `Eval Loss` (0.59) 远低于 `Train Loss` (1.60)，说明强数据增强（Mixup/Cutmix）有效防止了过拟合，模型泛化能力极强。
  
  
  ## 2025-11-20 (Day 2): 高光谱图像 (HSI) 迁移学习实验

### 1. 实验目标
- **任务**: 验证 FastViT 在高光谱图像 (Indian Pines) 上的分类性能。
- **核心对比**: 探究 "从零训练" (Scratch) 与 "加载 ImageNet 蒸馏预训练权重" (Fine-tuning) 的性能差异。

### 2. 实验数据集与预处理 (Dataset & Preprocessing)
- **数据集**: Indian Pines (IP)
  - **来源**: AVIRIS 传感器 (美国印第安纳州农业区)。
  - **规格**: 145x145 像素，20m 空间分辨率。
  - **波段**: 原始 200 有效波段 (光谱范围 0.4-2.5 µm)。
  - **类别**: 16 类地物 (主要是玉米、大豆等农作物)。
- **预处理策略**:
  - **降维**: 使用 PCA 将 200 波段压缩至 **3 主成分** (保留主要空间特征以适配 RGB 模型)。
  - **切片**: 采用 32x32 滑动窗口生成 Patch。
  - **划分**: 训练集/验证集 (80% / 20%)。

### 3. 实验结果对比

#### 实验 C: Baseline (从零训练)
- **实验名称**: PCAHSI
- **策略**: Scratch Training (无预训练)
- **最佳 Top-1 准确率**: 98.20%
- **最低 Eval Loss**: 0.335
- **结果分析**: 
  - 模型在训练前期 (Epoch 0-30) 收敛极其缓慢，存在明显的冷启动阶段。
  - 尽管最终精度尚可，但训练过程不稳定，依赖长周期的训练才能提取有效特征。

#### 实验 D: 迁移学习 (蒸馏权重)
- **实验名称**: PCAHSI_Fine-tuning
- **策略**: Fine-tuning (加载 ImageNet-1K 蒸馏权重)
- **最佳 Top-1 准确率**: 99.90%
- **最低 Eval Loss**: 0.216
- **结果分析**: 
  - **收敛速度极快**: 准确率曲线呈平滑上升趋势，无冷启动延迟。
  - **精度提升**: 相比 Baseline 提升了 1.7%，达到了近乎完美的 99.90%。
  - **结论**: 证明了 FastViT 在 ImageNet 上学到的纹理和空间特征，在经过 PCA 降维的高光谱数据上具有极强的迁移能力。


#### 实验 E: 挑战学术界标准 (1:9 极小样本划分)
- **实验名称**: PCAHSI_Fine-tuning_1_9
- **数据集划分**: 训练集 10% / 测试集 90% (符合 SpectralFormer 等 SOTA 论文标准)
- **策略**: Fine-tuning (加载 ImageNet 蒸馏权重)
- **最终结果 (Epoch 59)**:
  - **Top-1 准确率**: 30.02%
  - **Eval Loss**: 2.40 (仍呈快速下降趋势)
- **结果分析与重要发现**:
  1.  **精度骤降**: 相比 8:2 划分的 99.9%，在 1:9 划分下精度跌至 30%。这表明在极小样本情况下，仅靠 PCA 降维后的 3 通道空间特征已不足以支撑分类决策。
  2.  **欠拟合 (Underfitting)**: 训练/验证曲线在 Epoch 60 结束时仍处于陡峭上升期，未见收敛拐点。说明在样本极少时，模型需要更长的迭代周期来寻找最优解。
  3.  **光谱信息缺失**: PCA 将 200 波段压缩为 3 波段，丢失了关键的光谱指纹信息。在样本充足时(80%)模型可靠空间纹理“死记硬背”，但在样本稀缺时(10%)，光谱信息的缺失成为了致命瓶颈。


## 2025-11-21 (Day 3): 攻克少样本难题 —— 空谱联合训练与 SOTA 复现

### 1. 实验背景与目标
针对 Day 2 中 PCA-3 通道策略在 10% 极小样本下准确率仅为 30% 的瓶颈，本日旨在通过引入光谱维度和 SOTA 训练策略，在学术界标准的严苛条件下冲击高精度。

### 2. 技术路线重构 (Methodology)
- **输入层重构**: 修改 FastViT 源码 (`fastvit_hsi.py`)，将输入通道从硬编码的 3 扩展为动态可变的 `in_chans=30`。
- **数据管道升级**: 
  - 放弃图像格式，改用 `.npy` 张量格式存储，保留 PCA 降维后的前 30 个主成分 (覆盖 99.9% 光谱信息)。
  - 严格执行 **1:9 (10% Training)** 的学术界高难度划分标准。
- **训练策略升级**: 编写 `train_hsi_pro.py`，引入 Mixup, Cutmix, Native AMP 等高级特性。

### 3. 实验结果对比 (Results)

#### 实验 F: 30通道基础训练 (Baseline) 
- **实验名称**: PCA30_10p_Attack
- **配置**: 30 Channel Input, 10% Data, No Mixup/Cutmix.
- **最佳 Top-1 准确率**: 90.72%
- **Train Loss**: ~0.0001 (严重过拟合)
- **结果分析**: 
  - 相比 Day 2 的 3 通道实验 (30%)，引入光谱信息直接带来了 60% 的性能飞跃。
  - 但模型迅速陷入过拟合，训练集 Loss 降至 0，验证集 Loss 停滞不前，泛化能力受限。

#### 实验 G: SOTA 增强训练 (Advanced) 
- **实验名称**: PCA30_10p_SOTA_Mixup
- **配置**: 引入 Mixup (0.8), Cutmix (1.0), Drop Path (0.1), AMP.
- **最佳 Top-1 准确率**: **92.89%** 
- **Train Loss**: ~1.12 (保持较高，未过拟合)
- **结果分析**:
  - **精度提升**: 在 Baseline 90% 的高基准上进一步提升了 **2.17%**，达到 92.89%。
  - **机制验证**: 训练 Loss 维持在 1.1 左右，说明 Mixup 成功增加了训练难度，迫使模型学习鲁棒的光谱特征而非死记硬背。验证集 Loss (0.42) 远低于训练集 Loss，证明了极强的泛化能力。
  - **注**: 由于训练轮数 (100 Epochs) 相对样本量 (1000张) 过短，EMA 模型未能及时收敛 (23.9%)，最终成绩取自主模型。

