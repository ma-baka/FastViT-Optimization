## Day 1: 环境搭建与模型初步探索

### 1. 环境与数据排坑 (Setup & Debugging)
- **平台**: AutoDL (RTX 4090 D / 24GB VRAM)
- **环境修复**:
  - 解决 NumPy 2.x 与 PyTorch 1.11 不兼容报错，降级 NumPy。
  - 解决 DataLoader 报错：发现并删除了数据集中隐藏的 .ipynb_checkpoints 文件夹。
- **数据**: 成功配置 Imagenette (10类) 数据集。
- **工具**: 配置 GitHub SSH Key 实现代码同步；配置 .gitignore 过滤大文件。

### 2. 实验 A: 快速试错 (Baseline)
- **模型**: fastvit_t8 (Tiny版)
- **参数**: Batch Size 128, 从零训练 (Scratch)。
- **现象**:
  - 训练初期准确率极低 (~10%)。
  - **WandB 可视化**: 成功接入 WandB，观察到 train_loss 呈下降趋势，验证代码无 Bug，但模型收敛缓慢。

## 3. 实验 B: 性能压榨与模型升级 (Optimization)
- **模型**: `fastvit_ma36`
- **配置**: 
  - Batch Size: 64
  - Epochs: 100 + 10 (Cooldown)
  - 策略: 从零训练 (Training from Scratch)
- **最终结果 (Epoch 109)**:
  - **Top-1 准确率**: **87.18%**
  - **Top-5 准确率**: 98.73%
  - **Eval Loss**: 0.594
- **结果分析**:
  - 相比 T8 模型 (~10%) 提升巨大，证明了 MA36 架构在特征提取上的强悍能力。
  - 曲线显示在最后 10 个 Cooldown Epochs 阶段，准确率有显著拉升，证明移除数据增强后的微调策略非常有效。
  - `Eval Loss` (0.59) 远低于 `Train Loss` (1.60)，说明强数据增强（Mixup/Cutmix）有效防止了过拟合，模型泛化能力极强。
  
  
  ## Day 2: 高光谱图像 (HSI) 迁移学习实验

### 1. 实验目标
- **任务**: 验证 FastViT 在高光谱图像 (Indian Pines) 上的分类性能。
- **核心对比**: 探究 "从零训练" (Scratch) 与 "加载 ImageNet 蒸馏预训练权重" (Fine-tuning) 的性能差异。

### 2. 实验数据集与预处理 (Dataset & Preprocessing)
- **数据集**: Indian Pines (IP)
  - **来源**: AVIRIS 传感器 (美国印第安纳州农业区)。
  - **规格**: 145x145 像素，20m 空间分辨率。
  - **波段**: 原始 200 有效波段 (光谱范围 0.4-2.5 µm)。
  - **类别**: 16 类地物 (主要是玉米、大豆等农作物)。
- **预处理策略**:
  - **降维**: 使用 PCA 将 200 波段压缩至 **3 主成分** (保留主要空间特征以适配 RGB 模型)。
  - **切片**: 采用 32x32 滑动窗口生成 Patch。
  - **划分**: 训练集/验证集 (80% / 20%)。

### 3. 实验结果对比

#### 实验 C: Baseline (从零训练)
- **实验名称**: PCAHSI
- **策略**: Scratch Training (无预训练)
- **最佳 Top-1 准确率**: 98.20%
- **最低 Eval Loss**: 0.335
- **结果分析**: 
  - 模型在训练前期 (Epoch 0-30) 收敛极其缓慢，存在明显的冷启动阶段。
  - 尽管最终精度尚可，但训练过程不稳定，依赖长周期的训练才能提取有效特征。

#### 实验 D: 迁移学习 (蒸馏权重)
- **实验名称**: PCAHSI_Fine-tuning
- **策略**: Fine-tuning (加载 ImageNet-1K 蒸馏权重)
- **最佳 Top-1 准确率**: 99.90%
- **最低 Eval Loss**: 0.216
- **结果分析**: 
  - **收敛速度极快**: 准确率曲线呈平滑上升趋势，无冷启动延迟。
  - **精度提升**: 相比 Baseline 提升了 1.7%，达到了近乎完美的 99.90%。
  - **结论**: 证明了 FastViT 在 ImageNet 上学到的纹理和空间特征，在经过 PCA 降维的高光谱数据上具有极强的迁移能力。


#### 实验 E: 挑战学术界标准 (1:9 极小样本划分)
- **实验名称**: PCAHSI_Fine-tuning_1_9
- **数据集划分**: 训练集 10% / 测试集 90% (符合 SpectralFormer 等 SOTA 论文标准)
- **策略**: Fine-tuning (加载 ImageNet 蒸馏权重)
- **最终结果 (Epoch 59)**:
  - **Top-1 准确率**: 30.02%
  - **Eval Loss**: 2.40 (仍呈快速下降趋势)
- **结果分析与重要发现**:
  1.  **精度骤降**: 相比 8:2 划分的 99.9%，在 1:9 划分下精度跌至 30%。这表明在极小样本情况下，仅靠 PCA 降维后的 3 通道空间特征已不足以支撑分类决策。
  2.  **欠拟合 (Underfitting)**: 训练/验证曲线在 Epoch 60 结束时仍处于陡峭上升期，未见收敛拐点。说明在样本极少时，模型需要更长的迭代周期来寻找最优解。
  3.  **光谱信息缺失**: PCA 将 200 波段压缩为 3 波段，丢失了关键的光谱指纹信息。在样本充足时(80%)模型可靠空间纹理“死记硬背”，但在样本稀缺时(10%)，光谱信息的缺失成为了致命瓶颈。


## Day 3: 攻克少样本难题 —— 空谱联合训练与 SOTA 复现

### 1. 实验背景与目标
针对 Day 2 中 PCA-3 通道策略在 10% 极小样本下准确率仅为 30% 的瓶颈，本日旨在通过引入光谱维度和 SOTA 训练策略，在学术界标准的严苛条件下冲击高精度。

### 2. 技术路线重构 (Methodology)
- **输入层重构**: 修改 FastViT 源码 (`fastvit_hsi.py`)，将输入通道从硬编码的 3 扩展为动态可变的 `in_chans=30`。
- **数据管道升级**: 
  - 放弃图像格式，改用 `.npy` 张量格式存储，保留 PCA 降维后的前 30 个主成分 (覆盖 99.9% 光谱信息)。
  - 严格执行 **1:9 (10% Training)** 的学术界高难度划分标准。
- **训练策略升级**: 编写 `train_hsi_pro.py`，引入 Mixup, Cutmix, Native AMP 等高级特性。

### 3. 实验结果对比 (Results)

#### 实验 F: 30通道基础训练 (Baseline) 
- **实验名称**: PCA30_10p_Attack
- **配置**: 30 Channel Input, 10% Data, No Mixup/Cutmix.
- **最佳 Top-1 准确率**: 90.72%
- **Train Loss**: ~0.0001 (严重过拟合)
- **结果分析**: 
  - 相比 Day 2 的 3 通道实验 (30%)，引入光谱信息直接带来了 60% 的性能飞跃。
  - 但模型迅速陷入过拟合，训练集 Loss 降至 0，验证集 Loss 停滞不前，泛化能力受限。

#### 实验 G: SOTA 增强训练 (Advanced) 
- **实验名称**: PCA30_10p_SOTA_Mixup
- **配置**: 引入 Mixup (0.8), Cutmix (1.0), Drop Path (0.1), AMP.
- **最佳 Top-1 准确率**: **92.89%** 
- **Train Loss**: ~1.12 (保持较高，未过拟合)
- **结果分析**:
  - **精度提升**: 在 Baseline 90% 的高基准上进一步提升了 **2.17%**，达到 92.89%。
  - **机制验证**: 训练 Loss 维持在 1.1 左右，说明 Mixup 成功增加了训练难度，迫使模型学习鲁棒的光谱特征而非死记硬背。验证集 Loss (0.42) 远低于训练集 Loss，证明了极强的泛化能力。
  - **注**: 由于训练轮数 (100 Epochs) 相对样本量 (1000张) 过短，EMA 模型未能及时收敛 (23.9%)，最终成绩取自主模型。

## Day 4: 架构创新 —— RepSS-Mixer 空谱分离网络

### 1. 实验动机 (Motivation)
- **痛点**: Day 3 的实验表明，虽然 SOTA 训练策略 (Mixup) 能提升泛化性，但 FastViT 原生的 2D 卷积架构将 30 个光谱通道视为普通的颜色通道，忽略了光谱维度的序列相关性，导致精度在 93% 左右遇到瓶颈。
- **假设**: 解耦空间特征与光谱特征的提取过程，引入专门针对光谱维度的 1D 卷积，能更有效地捕捉高光谱数据的物理特性。

### 2. 架构改进 (Method: RepSS-Mixer)
设计并实现了 **RepSS-Mixer (Reparameterizable Spatial-Spectral Mixer)** 模块，替换原有的 RepMixer：
- **串行解耦**: 将原本的混合操作分解为两步串行处理：
  1.  **Spatial Mixing**: 使用 2D Depthwise Conv 提取空间纹理 (保持不变)。
  2.  **Spectral Mixing (New)**: 新增 `RepSpectralConv` 模块，将特征图重塑为序列，利用 **1D 卷积** 在光谱维度上滑动提取特征。
- **重参数化设计**:
  - **训练态**: 采用多分支结构 (Identity + 3x1 Conv + 5x1 Conv) 增强特征捕获能力。
  - **推理态**: 数学等价融合为单分支 5x1 Conv，实现**零推理成本增加**。

### 3. 实验结果对比 (Ablation Study)
*实验设置*: 所有组别均采用 10% 训练集、30通道 PCA 输入、以及相同的 SOTA 训练策略 (Mixup=0.8, Cutmix=1.0, EMA, AMP)。

| 实验组 | 模型架构 | 训练策略 | Top-1 Acc | Eval Loss | 结论 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Experiment F** | FastViT-MA36 (Original) | Basic | 90.66% | 0.445 | Baseline (过拟合) |
| **Experiment G** | FastViT-MA36 (Original) | SOTA (Mixup) | 92.89% | 0.429 | 策略有效，但架构受限 |
| **Experiment H** | **FastViT-RepSS (Ours)** | **SOTA (Mixup)** | **96.96%** | **0.307** | **SOTA Level (最佳)** |

### 4. 结果分析
- **显著提升**: 在完全相同的训练配置下，RepSS 架构比原始架构提升了 **4.07%**。这有力地证明了“空谱分离”及“1D 光谱卷积”在高光谱分类中的有效性。
- **收敛性**: 验证集 Loss 从 0.43 降至 0.30，下降幅度显著，说明模型真正学到了数据的物理规律，而非单纯拟合噪声。


## Day 5: 回归物理本质 —— 原始波段选择实验 (Final SOTA)

### 1. 实验动机 (Motivation)
- **反思**: Day 4 虽然利用 PCA 数据取得了 96.96% 的高分，但 PCA 破坏了光谱的物理序列性 (Spectral Sequence)，使得 RepSS-Mixer 中的 1D 光谱卷积实际上是在处理统计特征而非物理光谱。
- **目标**: 放弃 PCA，改用 **波段选择 (Band Selection)** 策略，直接输入具有物理连续性的原始光谱波段，验证模型在“真实噪声环境”下的鲁棒性。

### 2. 实验设置 (Setup)
- **数据**: Indian Pines (1:9 划分)。
- **预处理**: 均匀选取 30 个原始波段 (Uniform Band Selection)，保留物理波长顺序。
- **模型**: FastViT-RepSS (Day 4 同款架构)。
- **策略**: SOTA (Mixup + EMA + AMP)，控制所有超参数与 Day 4 完全一致。

### 3. 实验结果对比 (Results)

| 实验组 | 数据源 | 光谱特性 | 训练稳定性 | Top-1 Acc | Eval Loss |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Day 4** | PCA (30ch) | 强去噪，无物理顺序 | 极稳 | 96.96% | 0.307 |
| **Day 5** | **Raw Bands (30ch)** | **含噪声，有物理顺序** | 震荡 | **97.96%**  | **0.259** |

### 4. 现象分析与最终结论
- **震荡即收益 (Volatility is Gain)**: Day 5 的训练曲线呈现锯齿状震荡，这是模型在对抗原始传感器噪声的表现。
- **物理意义的胜利**: 尽管存在噪声，但原始波段保留了光谱曲线的连续性 (Continuity)。**RepSS-Mixer 的 1D 光谱卷积成功捕捉到了这些细微的光谱波形特征**，从而实现了比 PCA 更高的精度 (+1.0%) 和更低的 Loss (-0.048)。


## Day 6: 评价体系升级与预处理策略消融

### 1. 实验目标
- **体系升级**: 废弃原有的 Top-1/Top-5 指标，重构验证代码，引入高光谱领域标准的 **OA (Overall Accuracy)**, **AA (Average Accuracy)**, **Kappa Coefficient**。
- **预处理探究**: 在新的评价体系下，对比 **Raw Bands (原始波段)** 与 **Group-wise PCA (分组降维)** 的性能差异。

### 2. 代码重构与权衡 (Code Refactoring & Trade-offs)
为了计算 SOTA 指标，对 `train_repss_pro.py` 进行了核心逻辑修改：
- **验证机制变更**:
  - **旧版 (Batch-wise)**: 每跑一个 Batch 计算一次平均准确率。优点是速度快、省内存；缺点是无法计算全局指标 (Kappa/AA)。
  - **新版 (Global-wise)**: 在验证阶段将所有测试样本的预测结果存储在内存中，待遍历结束后统一调用 `sklearn` 计算。
  - **权衡**: 虽然增加了少量的内存开销，但保证了评价指标的数学严谨性和学术可比性。

### 3. 实验结果对比 (Ablation Study)
*配置*: FastViT-RepSS, 1:9 划分, Mixup+EMA.

| 预处理策略 | OA (总体) | AA (平均) | Kappa | 训练稳定性 |
| :--- | :--- | :--- | :--- | :--- |
| **A. Raw Bands (30ch)** | **98.33%** | 95.93% | 98.10 | 极稳 |
| **B. Group PCA (30ch)** | **98.33%** | **97.54%** | 98.10 | 剧烈震荡 |

### 4. 深度分析
- **精度与均衡性的权衡**: 
  - **Raw Bands** 表现出极佳的训练稳定性，Loss 下降平滑，适合作为稳健的 Baseline。
  - **Group PCA** 虽然训练曲线震荡严重（信噪比波动大），但在 **AA (平均准确率)** 上比 Raw Bands 高出 **1.6%**。这说明分组 PCA 的去噪能力有效帮助模型识别了**极少样本的困难类别**。
- **结论**: 
  - 如果追求系统稳定性，首选 **Raw Bands**。
  - 如果追求极致的类别均衡（不偏科），**Group PCA** 是更优解。
  - 考虑到整体性能，本项目后续将以 **Raw Bands** 为主基调，以保持实验的可复现性和稳定性。

### 5. 当前 SOTA 成绩
- **OA**: 98.33%
- **AA**: 97.54% (Best via Group PCA)
- **Kappa**: 0.981


## Day 7: 空间尺度消融与去步长手术 (Stride Surgery)

### 1. 实验动机
- **背景**: HSI 领域 SOTA 模型通常使用 $13 \times 13$ 甚至更小的切片以减少邻域噪声。
- **挑战**: FastViT 原生 32倍下采样导致无法处理小切片。
- **方案**: 对 FastViT 进行 **Stride Surgery (去步长手术)**，移除 Stem 和 Stage 1 的下采样，使其适配 $13 \times 13$ 输入。

### 2. 实验结果对比
*配置*: FastViT-RepSS, Raw Bands (30ch), Mixup+EMA, 10% 训练集.

| 输入尺寸 | 架构调整 | OA (总体) | AA (平均) | Kappa | 结论 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **$32 \times 32$** | 原版 (32x下采样) | **98.33%** | 95.93% | **0.9810** | 宏观纹理强，大类识别准 |
| **$13 \times 13$** | **魔改 (8x下采样)** | 98.25% | **96.64%** | 0.9801 | **类别更均衡，抗噪性强** |

### 3. 现象分析与最终结论
- **AA 的逆袭**: 虽然 $13 \times 13$ 模型的 OA 微弱落后 (-0.08%)，但其 **AA (平均准确率)** 却反超了 **0.71%**。这证明缩小切片范围有效减少了邻域噪声对**小样本类别**和**边界像素**的干扰。
- **Loss 的胜利**: $13 \times 13$ 模型最终取得了更低的验证集 Loss (0.247)，表明其泛化边界更为收敛。
- **总结**: 
  - 如果追求整体刷分 (OA)，保留大 Context ($32 \times 32$) 是优选。
  - 如果追求精细化分类和类别公平性 (AA)，经过去步长手术的 $13 \times 13$ 模型更胜一筹。
  - 本项目成功证明了 FastViT 架构具有极强的**尺度适应性 (Scale Adaptability)**。